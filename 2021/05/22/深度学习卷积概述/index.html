

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#3cbdfe">
  <meta name="description" content="就硬卷...">
  <meta name="author" content="CAIWEI">
  <meta name="keywords" content="">
  <meta name="description" content="就硬卷...">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习卷积概述">
<meta property="og:url" content="http://example.com/2021/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF%E6%A6%82%E8%BF%B0/index.html">
<meta property="og:site_name" content="CAIWEI的博客">
<meta property="og:description" content="就硬卷...">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/paper/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%83%8F%E9%87%87%E6%A0%B7%E7%BB%BC%E8%BF%B0(1.jpg">
<meta property="og:image" content="http://deeplearning.stanford.edu/wiki/images/f/f9/Autoencoder636.png">
<meta property="og:image" content="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/image-20200408004128834.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122143230026.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122143341925.png">
<meta property="og:image" content="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/fsafsa13213.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122143517475.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122143601820.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200220230126232.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d3cGppYXlvdQ==,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/shendu153135.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/20190728153423546.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122143755317.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122144623327.png">
<meta property="og:image" content="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/imsafma134.jpg">
<meta property="og:image" content="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/sadasd315164.jpg">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2019080116323060.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122144815796.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122144856779.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122144934656.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122145040963.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122145107859.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122145141623.png">
<meta property="og:image" content="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/41a59b4bd87f50222e05277057b34ce.png">
<meta property="article:published_time" content="2021-05-22T06:24:23.000Z">
<meta property="article:modified_time" content="2021-11-30T06:09:59.255Z">
<meta property="article:author" content="CAIWEI">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/paper/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%83%8F%E9%87%87%E6%A0%B7%E7%BB%BC%E8%BF%B0(1.jpg">
  
  <title>深度学习卷积概述 - CAIWEI的博客</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="CAIWEI的博客" type="application/atom+xml">
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong> </strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/0.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="深度学习卷积概述">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-05-22 14:24" pubdate>
        2021年5月22日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      24k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      74 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">深度学习卷积概述</h1>
            
            <div class="markdown-body">
              <p>就硬卷...</p>
<a id="more"></a>
<h2 id="摘要">摘要</h2>
<p>深度学习是机器学习和人工智能研究的最新且最热门的趋势之一。深度学习方法为计算机视觉带来了革命性的进步，新的深度学习理论和技术正在不断诞生，在计算机视觉领域超越传统的机器学习甚至是现有的深度学习方法。近年来，有关深度学习方面的研究成果爆炸式地增长，由于其发展迅猛，导致了它的进展很难被新的研究者入手并跟进。此外，深度学习已有的方法数量上过于庞大，种类上过于繁多，虽然有大量的深度学习综述为研究者提供现有的研究发展情况，但是其往往是全面地介绍整个深度学习方法。因而，本文将着重于计算机视觉领域，从深度学习方法中的图像采样对深度学习展开叙述。</p>
<h2 id="序言">1.序言</h2>
<p>深度学习（DeepLearning，DL）被认为是机器学习（Machine Learning ，ML）子领域，其采用多层次的非线性信息处理和抽象，用于有监督、无监督、半监督、自监督、弱监督等的特征学习、表示、分类、回归和模式识别等[1-01]。在 2000 年时深度学习的方法被用于人工神经网络（ANN），随着层数的加深，人工神经网络的被单独提出深层神经网络（DNN）这一分支。在2012年 ImageNet大规模视觉识别挑战赛上卷积神经网络AlexNet获得冠军，并且误判率令人惊艳[1-02]，从此卷积神经网络受到众多研究者的瞩目，并促使了深度学习的快速发展，目前计算机视觉（CV）领域多使用卷积神经网络，并且神经网络深度逐渐加深[1-03]。在自然语言处理（NLP）领域，由于循环神经网络（RNN）结构使得其较适合处理文本分析等场景从而受到众多研究者的欢迎。生成对抗网络以零和博弈的思想依赖生成模型和判别模型使得在图像生成方面效果良好。此外，较为常用的深度学习方法还有自编码器[1-04]、胶囊网络[1-05]、属于循环神经网络的长短记忆网络[1-06]、增强神经网络、深度信念网络、 深度朗伯网络、循环支持向量机等[1-07]。</p>
<p>对于图像分类、处理任务，神经网络中一大部分操作是采样，尤其是卷积神经网络。而神经网络的采样操作中较为常见的为卷积和池化，另外还有反卷积、反池化等多种多样的采样操作。各种采样的目的与原理都不尽相同，正是这些不一样的采样构成了神经网络结构中重要的部分。</p>
<h2 id="上采样与下采样">2.上采样与下采样</h2>
<h3 id="上采样">上采样</h3>
<p>在计算机视觉领域，输入图像通过卷积核提取特征后，输出的尺寸可能会变小，虽然在图像分类的是没有需要将图像再次恢复到原来的尺寸，但是图像的语义分割等却是需要恢复图像尺寸以便进行进一步计算。这种扩大图像尺寸，实现图像从小分辨率提升到大分辨率的映射操作，叫做上采样(Upsample)。 目前常见的上采样有插值法、反卷积、反池化，此外深度学习中的上采样方法还有 ： 扩张（空洞）卷积（dilated conv） 、PixelShuffle 、 DUpsampling 、 Meta-Upscale 、 CAPAFE等。</p>
<p>其中插值的方法，即在原图像已有的像素点之间插入新的像素点。目前已有的插值方法包括：双线内插法、 三次内插法 、反距离加权插值法、克里金插值法、最小曲率法、谢别德法、自然邻点插值法、最近邻点插值法、多元回归法、径向基函数法、线性插值三角网法、移动平均法、局部多项式法、 距离倒数乘方法 、 自然邻点插值法 、 拉格朗日插值法、牛顿均差插值法、分段线性插值、分段三次Hermite插值、三次样条插值[2-01~10]......插值的在超分辨率图像重建等方面有重要的应用。</p>
<p>反卷积和反池化等则常在卷积神经网络中出现。</p>
<h3 id="下采样">下采样</h3>
<p>下采样（subsampled）也称为降采样downsampled），其采样层是使用池化的相关技术来实现，目的是降低维度并保留有效特征，可在一定程度避免过拟合。</p>
<p>池化属于降采用的一种，池化除了降低维度，其目的还有保持旋转、平移、伸缩不变形，增大感受野等。采样有最大值采样，平均值采样，求和区域采样和随机区域采样等；池化也类似地有最大值池化，平均值池化，随机池化，求和区域池化等。</p>
<p>降采样原理：对一个尺寸大小为M * N地图像 ，对其进行 s倍下采样，即得到<span class="math inline">\(\frac{M * N}{s^2}\)</span>分辨率地图像，其中<span class="math inline">\(s\)</span>是<span class="math inline">\(M\)</span>,<span class="math inline">\(N\)</span>的公约数。具体对于特征图，则是把原始图像像素点为<span class="math inline">\(s*s\)</span>的区域的变成一个像素，这个像素点的值就是该区域内所有像素的均值：<span class="math inline">\(p_k=\sum_{i \in win(k)}I_i/s^2\)</span></p>
<p>降采样的作用主要有：</p>
<ol type="1">
<li><p>保持平移、旋转等的不变性；</p></li>
<li><p>降维的同时保留主要特征，减少计算量，防止过拟合，提高模型泛化能力。</p></li>
</ol>
<p>卷积神经网络中常见的降采样通过是池化层实现，目前常见的池化层有：最大池化层，平均池化层等。池化目的是对输入的特征图（Feature Map）进行降维以减少参数并且保留有效的特征，以减少之后的运算量。</p>
<center>
<img src="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/paper/深度学习图像采样综述(1.jpg" srcset="/img/loading.gif" lazyload="" width="600">
</center>
<h2 id="卷积神经网络中的图像采样">3.卷积神经网络中的图像采样</h2>
<p>近几年来随着深度学习的迅速发展，卷积神经网络在计算机视觉领域理论与应用快速发展。2012年AlexNet[3-01]在ImageNet 大规模图像分类挑战赛的成绩让众多研究者看到神经网络在计算机视觉上的历史性突破，由此掀起了卷积神经网络的研究热潮，深度学习也从学术界到业界逐渐被大众熟知。之后，新的神经网络不断出现，著名的有牛津大学视觉几何组在2014年提出的VGGNet [3-02]、由谷歌研究人员提出的GoogleNet [3-03] ；2016年由何凯明等人提出的ResNet [3-04] 使神经网络超过上百层且效果更佳，并衍生出了ResNeXt、DenseNet 、NPN、SENet等；在2018年CVPR会议，张婷等人延续IGCV 1提出了IGCV 2 和IGCV 3 [3-05]。如今卷积神经网络在图像分类、语义分割、目标检测、目标追踪、行为识别等领域都有了可观的应用成果[3-06]。</p>
<p>典型的卷积神经网络一般包括输入层、输出层以及诸多的隐藏层，其中隐藏层一般由卷积层、池化层、全连接层、等构成，其中卷积以及池化其本质是图像采用。卷积神经网络最具有特色的结构便是卷积核，由于卷积才使得卷积神经网络在处理图像上相对于循环神经网络等效果较好。卷积和池化是提取特征和降维的过程，特征图经过数层的卷积运算最终得到的是较为高级的特征，因而，如何利用有限的计算资源提取出更多有效的高级特征便是深度学习在计算机视觉领域研究内至关重要的研究。</p>
<p>卷积神经网络中的图像采样主要表现在卷积以及池化，卷积主要目的在于提取特征，而池化的目的在于降维减少计算量。输入层读入规则化的图像，每一层的每个神经元将前一层的一组小的局部感受野并权值共享。神经元抽取边缘、角点等基本的视觉特征，经过一层层的采样逐渐转变为高级视觉特征。卷积神经网络通过卷积操作获得特征图，每个位置，来自不同特征图的单元得到各自不同类型的特征。一个卷积层中可能包含多个不同权值的特征图，这使得被保留的图像具备更丰富的特征。卷积层后边会连接池化层进行降采样操作，一方面可以降低图像的分辨率，减少参数量，另一方面可以获得平移和形变的鲁棒性。卷积层和池化层的交替分布，使得特征图的数目逐步增多，而且分辨率逐渐降低，最终完成分类。</p>
<h3 id="卷积自编码器">3.1 卷积自编码器</h3>
<p>无监督学习方法的主要目的是从未标记的数据中提取一般有用的特征，检测和去除输入冗余，并在稳健和有区别的表示中仅保留数据的基本方面。卷积自编码器是一种结合了卷积和池化操作的无监督学习方法，其通过特征提取、堆栈，实现深层的神经网络[3.1-1]。</p>
<center>
<img src="http://deeplearning.stanford.edu/wiki/images/f/f9/Autoencoder636.png" srcset="/img/loading.gif" lazyload="" width="300">
</center>
<center>
图 卷积自编码器
</center>
<p>假设有k个卷积核，每个卷积核由参数<span class="math inline">\(w^k\)</span>和<span class="math inline">\(b^k\)</span>组成，用和<span class="math inline">\(h^k\)</span>表示卷积层，则 <span class="math display">\[
h^k=σ(x∗w^k+b^k)
\]</span> 将得到的<span class="math inline">\(h^k\)</span>进行特征重构，可以得到下式： <span class="math display">\[
y=σ(h^k∗w^k+c)
\]</span></p>
<p>将输入的样本和最终利用特征重构得出来的结果进行欧几里得距离比较，通过BP算法进行优化，就可以得到一个完整的卷积自编码器（CAE）: <span class="math display">\[
E=\frac{1}{2n}∑(x_i−y_i)^2
\]</span></p>
<h3 id="经典卷积神经网络">3.2 经典卷积神经网络</h3>
<h4 id="lenet">3.2.1 LeNet</h4>
<figure>
<img src="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/image-20200408004128834.png" srcset="/img/loading.gif" lazyload="" alt="image-20200408004128834"><figcaption aria-hidden="true">image-20200408004128834</figcaption>
</figure>
<p>从最早的卷积神经网络LetNet-5的结构图中可看出其具有两个卷积层和两个池化层（图中表示为降采样层），输入32 * 32 <em>3的图像，第一个卷积层将三个通道的RGB图按照卷积计算的规则提取低级特征得到6个28 </em> 28的特征图，第一个池化层对3 * 32 <em>32 的图像经过卷积运算后得到的的特征图进行采样，得到 6个14 </em> 14 的特征图。 这是神经网络中最早利用图像局部相关性的原理而对图像抽样，以达到减少数据处理量并保留有用信息。第二个池化层通过降采样将16个10 * 10的特征图转变为16个5 * 5的特征图。不难看出，LetNet-5的计算量都是经过降采样减少的，而图像特征则是经过卷积提取的。</p>
<h5 id="卷积">卷积</h5>
<p>卷积操作通过卷积核（Convolution Kernel ）实现，卷积核并非在深度学习中最早提出，早在边缘检测等研究例如Sobel 算子等滤波算子就已出现，当时被称为Filter[3.2.1-1]。卷积核具有的一局部性，即它只关注局部特征，局部的程度取决于卷积核的大小，其意义在于比较图像邻近像素的相似性，而图像相邻像素通常有一定的关联，故卷积这种采样方式很适合用于深度学习中对图像的处理。</p>
<p>时域卷积对应频域相乘，故原图像与卷积核的卷积，其实是对频域信息的选择。例如，图像中的边缘和轮廓属于是高频信息，图像中某区域强度的综合考量属于低频信息。</p>
<p>信号与图像处理中的一维信号的卷积： <span class="math display">\[
y[n]=x[n]*h[n]= \sum_k x[k]h[n-k]
\]</span> 其中 <span class="math inline">\(x[n]\)</span>是输入信号 <span class="math inline">\(h[n]\)</span>是单位响应。于是输出信号 <span class="math inline">\(y[n]\)</span>就是输入信号 <span class="math inline">\(x[n]\)</span>的延迟响应的叠加。这也就是一维卷积本质：加权叠加/积分。</p>
<p>那么对于二维信号，比如图像，卷积的公式为： <span class="math display">\[
y[m,n]=x[m,n]*h[m,n]=\sum_j \sum_i x[i,j]h[m-i,n-j]
\]</span> 若卷积核大小是3*3，则卷积核如图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122143230026.png" srcset="/img/loading.gif" lazyload=""></p>
<p>输入图像到输出图像的变化如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122143341925.png" srcset="/img/loading.gif" lazyload=""></p>
<p>二维卷积也有加权叠加/积分的作用,其中卷积核进行了水平和竖直方向的翻转。</p>
<p>kernel size（卷积核大小） =3 ,stride（步长）=1 的反卷积计算过程： $$ =[w_1 w_2 w_3]^T  =[x_1 x_2 x_3 x_4]^T \ *^T = </p>
<p></p>
<p>=</p>
<p>$$</p>
<h5 id="池化">池化</h5>
<p>池化层又成为降采样层或（下）欠采样层，其与卷积都是卷积神经网络中不可缺少的一部分，较早的卷积神经网络例如AlexNet通常会是卷积层后会紧跟一个池化层。池化的主要功能是对特征进行降维，减少参数量以降低计算量，避免过拟合，最大池化和平均池化是最为常见的两种池化方式。最大池化保留了图像纹理特征,平均池化保留了图像的整体数据特征，池化都达到了降维的作用。</p>
<p>以最大池化为例，其过程如图所示：</p>
<p><img src="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/fsafsa13213.jpg" srcset="/img/loading.gif" lazyload=""></p>
<p>和卷积层类似，池化层也有<strong>窗口</strong>和<strong>步长</strong>的概念，其中<strong>步长</strong>在里面的作用也是完全相同的，就是窗口每次移动的像素个数。</p>
<h4 id="alexnet">3.2.2 AlexNet</h4>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122143517475.png" srcset="/img/loading.gif" lazyload=""></p>
<p>AlexNet是2012年 ImageNet大规模视觉识别挑战赛（ ILSVRC ）的冠军，正是它的出现让沉寂14年的卷积神经网络受到瞩目，因而促使了深度学习快速的快速发展。</p>
<p>因为显存的关系，AlexNet在两个GPU上训练，输入的是227 * 227 * 3的RGB图像。对于单个GPU，图像是经过卷积运算之后为55 * 55 <em>3，该神经网络总过对特征图经过三次最大池化（降采样）分别得到27 </em> 27 <em>96 ，13 </em> 13 * 256，6 * 6 * 256 。 相较于LeNet，AlexNet采用了分组卷积，其分布式计算使得图像特征得以分散在不同的计算设备上计算；另外其卷积步长不再完全为1，这种步长大于1的卷积被称之为跨步卷积。</p>
<h5 id="分组卷积-group-convolution">分组卷积（ Group convolution ）</h5>
<p>由于计算资源有限，AlexNet使用两个GPU训练模型，故卷积操作在两个GPU处理，特征图也在两个GPU上处理，最后把两个GPU的计算结果融合。</p>
<p>此后许多轻量级的SOTA网络（State Of The Art， 指在该项研究中当前最先进的网络结构）都用到了分组卷积，以完成多个计算设备使用有限的算力。</p>
<h5 id="跨步卷积strided-convolution">跨步卷积（Strided Convolution）</h5>
<p>相比LeNet-5，AlexNet的卷积操作开始出现充填(padding)和步长(stride)，此后的卷积神经网络中跨步卷积成了常见的卷积操作。</p>
<p>kernel size（卷积核大小） =3 ,stride（步长）&gt;1 的反卷积计算过程： <span class="math display">\[
\vec{w}=[w_1 \quad w_2 \quad w_3]^T \quad\vec{x}=[x_1 \quad x_2 \quad x_3 \quad x_4]^T \\\vec{w}*^T \vec{x}=\left[ \begin{matrix}  w_1 \quad w_2 \quad w_3 \quad 0 \quad 0 \quad 0\\ \\0 \quad w_1 \quad w_2 \quad w_3 \quad 0 \quad   \end{matrix}  \right]    \left[ \begin{matrix}0\\ x_1 \\x_2 \\x_3 \\x_4 \\0   \end{matrix}  \right]    =    \left[ \begin{matrix}  w_2 x_1+w_3x_2 \\  w_1x_2+w_2x_3+w_3x_4 \\      \end{matrix}  \right]
\]</span></p>
<p>当卷积stride（步长）&gt;1时可起到降采样的作用。</p>
<h4 id="zfnet">3.2.3 ZFNet</h4>
<h5 id="反卷积">反卷积</h5>
<p>反卷积（Deconvolution） 的概念首次出现在Zeiler在2010年发表的论文中[3.2.3-1]，而Zeiler在2011年发表的论文中反卷积作为术语正式使用[3.2.3-2]。ZFNet沿袭AlexNet的网络架构，并提出了有关优化性能的一些关键想法 ， 展示了将滤波器和权重可视化的正确方法 ， 反卷积可视化训练过程中特征的演化及发现潜在的问题[3.2.3-3]。随后反卷积在神经网络可视化上的成功应用的案例逐渐增多，例如场景分割、生成模型等的工作也逐渐采用反卷积。反卷积也有很多其他的叫法，比如：Transposed Convolution，Fractional Strided Convolution等。</p>
<p>反卷积是上采样的一种，由于其是通过转置卷积核来达到去除卷积的目的，因而也称为转置卷积(transpose convolution)或者分数步长卷积(convolution with fractional strides)或者后向卷积(backwards strided convolution)，其名称具有一定的争议性。 反卷积并非正向卷积的完全逆向，从目前的实现方法上看我们可以将其视作一种较特殊的正向卷积。反卷积按照一定的比例在图像边缘补0来扩大图像的尺寸，再旋转卷积核，最后进行正向卷积。</p>
<p>反卷积的数学表达：</p>
<ul>
<li>输入大小$ i i<span class="math inline">\(的图像，卷积核大小为\)</span> k k $，由图像边缘充填(padding) <span class="math inline">\(p\)</span> , 步长(strides) <span class="math inline">\(s\)</span> ，由卷积计算公式<span class="math inline">\(o = \frac{i+2p-k}{s}+1\)</span>得到输出图像大小为<span class="math inline">\(o \times o\)</span></li>
<li>输入$ i i<span class="math inline">\(的图像特征转变为列向量\)</span>X<span class="math inline">\(，输出\)</span> o o<span class="math inline">\(的图像特征展开成列向量\)</span>Y$ .</li>
<li>若是正向卷积，则由<span class="math inline">\(Y=CX\)</span>得到稀疏矩阵<span class="math inline">\(C\)</span>；若是反向卷积，则由<span class="math inline">\(Y=C^T X\)</span> 得到$ C $</li>
</ul>
<p>当卷积 stride &gt;1(步长大于1)时，可以实现降采样；类似的，反卷积也可以实现升采样。</p>
<p>kernel size（卷积核大小） =3 ,stride（步长）=1的反卷积计算过程： $$ =[w_1 w_2 w_3]^T  =[x_1 x_2 x_3 x_4]^T \ *^T = </p>
<p></p>
<p>=</p>
<p><span class="math display">\[
kernel size（卷积核大小） =3 ,stride（步长）&gt;1 的反卷积计算过程：
\]</span> =[w_1 w_2 w_3]^T  =[x_1 x_2 x_3]^T \ *^T = </p>
<p></p>
<p>=</p>
<p>$$</p>
<h5 id="反池化">反池化</h5>
<p>反池化是上采样的一种，ZFNet作者在论文中首次使用反池化。反池化是池化的逆操作，因为经过池化之后部分信息已丢失，因而想要以池化后的信息恢复到全部信息，则存只能通过补位来实现最大程度上的信息完整。</p>
<p>池化有两种：最大池化和平均池化，其反池化也需要与其对应。反池化无法直接实现，ZFNet作者在训练神经网络的过程中记录每个池化操作z*z的区域内输入最大值的位置，反池化时，将最大值返回其对应的位置，其余位置的数值补为0，由此近似得实现了反池化。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122143601820.png" srcset="/img/loading.gif" lazyload=""></p>
<figure>
<img src="https://img-blog.csdnimg.cn/20200220230126232.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d3cGppYXlvdQ==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" lazyload="" alt="在这里插入图片描述"><figcaption aria-hidden="true">在这里插入图片描述</figcaption>
</figure>
<h4 id="vggnet">3.2.4 VGGNet</h4>
<p>VGGNet为此前卷积神经网络的加深网络，VGGNet的层数更深，大量的卷积层更利于图像从低级特征提取到高级特征。VGGNet-19 训练输出的图像大小为 224 * 224 <em>3，经过第一次卷积之后的图像特征大小为 224 </em>224 * 64。VGGNet-19 一共有五个池化层，池化后的特征图大小分别是 112 * 112 * 65，56 * 56 * 128，28 * 28 <em>256，14 </em> 14 * 512，7 * 7 * 512.</p>
<p>VGGNet中每个最大池化层后的滤波器数量都增加一倍，因而加强了缩小空间尺寸并保持深度增长。在此前，如AlexNet的卷积大小为11<em>11，而在VGGNet中，卷积核大小都较小，至此已充分证明卷积核两个3 </em> 3jchang的卷积采样效果会比一个5 * 5的卷积核好，且计算量也相对较小，故此后的神经网络中除了一些特定的应用场景所需否则都一般采用较小的卷积核。</p>
<h4 id="googlenet">3.2.5 GoogLeNet</h4>
<h4 id="v1">V1</h4>
<p>GoogLeNet是 ImageNet 2014比赛分类的冠军，其错误率低至6.656%，相比此前的卷积神经网络，其大大增加了卷积神经网络的深度。 GoogLeNet Incepetion V1相比AlexNet的卷积神经网络更深，但是计算量却更少，准确率确远高于AlexNet。GoogLeNet参数少是相比其他神经网络计算量更小的直接原因，造成参数少的原因除了模型精心设计因而参数复用率高以外，另外用全局平均池化层代替了以往的网络结构中最后的全连接。</p>
<h4 id="v2">V2</h4>
<h4 id="v3">V3</h4>
<h4 id="resnet">3.2.6 ResNet</h4>
<p>ResNet（残差网络）在网络架构上参照了Inception V1，其快捷连接(shortcut connection)降低了池化层的重要性，以ResNet-34为例，从网络结构中可以清楚地看到只有第一次卷积以及倒数第二层池化层，此外便全是卷积层。通过恒等映射的方式解决神经网络准确率饱和退化的问题，因而这种网络结构可以达到远超过此前的深层次，并且除了输入输出的邻近层外都可以采用卷积核逐步提取图像特征，由此可以在保证增加层数并使用卷积将图像从低级特征逐步提取到高级特征，在不考虑降维的情况下无需在中间加入池化层。</p>
<p><img src="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/shendu153135.jpg" srcset="/img/loading.gif" lazyload=""></p>
<p>ResNet中当输入输出尺寸发生增加时，会考虑两个策略：（a）快捷连接仍然使用自身映射，对于维度的增加用零来填补空缺。此策略不会引入额外的参数；（b）投影捷径被用来匹配尺寸（由1×1的卷积完成）。对于这两种选项，当快捷连接在两个不同大小的特征图谱上出现时，用步长(stride)等于2来处理。在工程实现中常常采用了第二种解决方式，并且对于下采样操作(subsample)是通过1x1的池化来完成的。</p>
<p>ResNet最为重要的是恒等映射的思想，其后大量著名的神经网络借鉴或以其为模板，衍生出了：用不同长度子路径组合选择合适的子路径集合提高模型表达能力的FractalNet[3.6.2-1]； 以ResNetv2为基础增大残差块中的卷积核数量以此学习到更多有用信息的WideResNet[3.6.2-2]；每个分支都用相同的拓扑结构，通过变量基数达到控制组的数量目的的ResNeXt[3.6.2-3]；每一层和前面所有层相连并构建结构窄的网络，学习少量特征图以降低了冗余性的 DenseNet[3.6.2-4] ； 使用残差连接复用特征并大量抛弃最大池化避免高频高幅的DarkNet-53[3.6.2-5] ； 采用空洞卷积而非降采样的方法扩大感受野的 Dilated ResNet[3.6.2-6] 。</p>
<h4 id="dilated-resnet空洞卷积神经网络">3.2.7 Dilated ResNet（空洞卷积神经网络）</h4>
<p>空洞卷积（Dilated Convolution）也被称为扩展卷积或者膨胀卷积，是在2016年由 普林斯顿大学的 Fisher Yu 等人在其参考残差网络构建的空洞卷积神经网络中的一种上采样方法[3.2.7-1] 。扩张卷积是在标准卷积核中注入空洞，以此增加模型的感受野（reception field）。相比常规的卷积操作，扩张卷积增多了一个卷积核的点的间隔数量dilation rate这个参数，常规的卷积操作dilatation rate为1。 感受野的大小在目标检测和图像分割尤其重要，而感受野需要依靠下采样，然而图像经过下采样会导致小目标不容易被检测到；虽然多层特征图提取目标在此方面有一定程度的应对效果，，但是前面的特征图语义信息不够因而并不合适 ； 如果没有下采样，神经网络的计算量会增大，另外会因为没有逐渐将低级特征提取到高级特征导致效果不佳，感受野也不会有变化。而空洞卷积可以不牺牲特征图尺寸增大感受野[3-02]。空洞卷积的目的是扩大感受野，实际上并没有扩大特征图，因而并不能当作上采样方法。</p>
<h4 id="yolo2">3.2.8 YOLO2</h4>
<p>yolo2是继2015年Joseph Redmon等人提出yolo [3.28-1 ] 之后在2017提出的一种改进 [3-04] 。yolo在工业界应用广泛，大量目标检测的落地都依赖yolo实现。其中在yolo2中提到穿透层(passthrough layer)。</p>
<p>穿透层的本质是特征重排，26 * 26 * 512的特征图按行和列隔点采样，得到4个13 * 13 * 512的特征图并按通道顺序串联，得到13 * 13 * 2048的特征图.还有就是，穿透层不学习参数，其直接将前面的层的特征重排拼接到后面的层，故网络越在前面的层，感受野越小，因而有利于小目标的检测。</p>
<h4 id="senet">SENet</h4>
<p>SENet考虑特征通道之间的关系，其显式地建模特征通道之间的相互依赖关系。其采用了一种全新的特征重标定策略。 SENet通过学习的方式获取到每个特征通道的重要程度，再依照此重要程度提升有用的特征并且抑制对当前任务用处不大的特征，以此达到更高效并准确的完成图像分类。</p>
<h3 id="全卷积神经网络">3.3 全卷积神经网络</h3>
<p>全卷积神经网络是在2015年由UC Berkeley的Jonathan Long等提出的用于图像语义分割的一种卷积神经网络[3.3-1]。</p>
<p>普通的卷积神经网络结构适用于图像级别的分类和回归任务，因为其最后得到输入图像的分类的概率。特征图（feature map）在经过最后的卷积操作之后会经过若干个全连接层计算，被映射成为固定长度的特征向量以此完成分类。如AlexNet为完成ImageNet大规模图像分类挑战赛中一千种图像分类故而最后输出一个1000维的向量表示输入图像属于每一类的概率。 全卷积神经网络（FCN）对图像进行像素级的分类，其适用于语义级别的图像分割问题。相比经典的CNN在最后使用全连接层得到固定长度的特征向量进行分类不同，FCN可接受任意尺寸的输入图像，采用反卷积对最后一个卷基层的特征图（feature map）进行上采样，使它恢复到输入图像相同的尺寸，从而可以对每一个像素都产生一个预测，同时保留了原始输入图像中的空间信息，最后奇偶在上采样的特征图进行像素的分类。FCN从抽象的特征中恢复出每个像素所属的类别，即从图像级别的分类进一步延伸到像素级别的分类。 FCN将传统CNN中的全连接层转化成一个个的卷积层。在传统的CNN结构中，前5层是卷积层，第6层和第7层分别是一个长度为4096的一维向量，第8层是长度为1000的一维向量，分别对应1000个类别的概率。FCN将这3层表示为卷积层，卷积核的大小(通道数，宽，高)分别为（4096,7,7）、（4096,1,1）、（1000,1,1）。所有的层都是卷积层，故称为全卷积神经网络，如下图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/20190728153423546.png" srcset="/img/loading.gif" lazyload=""></p>
<p>其网络结构如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122143755317.png" srcset="/img/loading.gif" lazyload=""></p>
<h3 id="图卷积神经网络">3.4 图卷积神经网络</h3>
<p>图(Graph)是一种由若干个结点(Node)及连接两个结点的边(Edge)所构成的图形，用于刻画不同结点之间的关系。下面是一个生动的例子，图片来自论文[14]:</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122144623327.png" srcset="/img/loading.gif" lazyload="" alt="图像与图（示例）"><figcaption aria-hidden="true">图像与图（示例）</figcaption>
</figure>
<p>最早的图神经网络起源于Franco博士的论文[2], 它的理论基础是<strong>不动点</strong>理论。给定一张图 GG，每个结点都有其自己的特征(feature), 本文中用xvxv表示结点v的特征；连接两个结点的边也有自己的特征，本文中用x(v,u)x(v,u)表示结点v与结点u之间边的特征；GNN的学习目标是获得每个结点的图感知的隐藏状态 hvhv(state embedding)，这就意味着：对于每个节点，它的隐藏状态包含了来自邻居节点的信息。那么，如何让每个结点都感知到图上其他的结点呢？GNN通过<strong>迭代式更新</strong>所有结点的隐藏状态来实现，在t+1t+1时刻，结点vv的隐藏状态按照如下方式更新：</p>
<p>ht+1v=f(xv,xco[v],htne[v],xne[v]),𝐡𝑣t+1=𝑓(𝐱𝑣,𝐱𝑐𝑜[𝑣],𝐡𝑛t𝑒[𝑣],𝐱𝑛𝑒[𝑣]),</p>
<p>上面这个公式中的 ff 就是隐藏状态的<strong>状态更新</strong>函数，在论文中也被称为<strong>局部转移函数</strong>(local transaction function)。公式中的xco[v]𝐱𝑐𝑜[𝑣]指的是与结点vv相邻的边的特征，xne[v]𝐱𝑛𝑒[𝑣]指的是结点vv的邻居结点的特征，htne[v]𝐡𝑛t𝑒[𝑣]则指邻居结点在tt时刻的隐藏状态。注意 ff 是对所有结点都成立的，是一个全局共享的函数。那么怎么把它跟深度学习结合在一起呢？聪明的读者应该想到了，那就是利用神经网络(Neural Network)来拟合这个复杂函数 ff。值得一提的是，虽然看起来 ff 的输入是不定长参数，但在 ff 内部我们可以先将不定长的参数通过一定操作变成一个固定的参数，比如说用所有隐藏状态的加和来代表所有隐藏状态。我们举个例子来说明一下：</p>
<p><img src="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/imsafma134.jpg" srcset="/img/loading.gif" lazyload="" width="40%"></p>
<p>假设结点55为中心结点，其隐藏状态的更新函数如图所示。这个更新公式表达的思想自然又贴切：不断地利用当前时刻邻居结点的隐藏状态作为部分输入来生成下一时刻中心结点的隐藏状态，直到每个结点的隐藏状态变化幅度很小，整个图的信息流动趋于平稳。至此，每个结点都“知晓”了其邻居的信息。状态更新公式仅描述了如何获取每个结点的隐藏状态，除它以外，我们还需要另外一个函数 gg 来描述如何适应下游任务。举个例子，给定一个社交网络，一个可能的下游任务是判断各个结点是否为水军账号。</p>
<p>ov=g(hv,xv)𝐨𝑣=𝑔(𝐡𝑣,𝐱𝑣)</p>
<p>在原论文中，gg 又被称为<strong>局部输出函数</strong>(local output function)，与 f类似，g 也可以由一个神经网络来表达，它也是一个全局共享的函数。那么，整个流程可以用下面这张图表达：</p>
<p><img src="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/sadasd315164.jpg" srcset="/img/loading.gif" lazyload=""></p>
<p>仔细观察两个时刻之间的连线，它与图的连线密切相关。比如说在 T1T1 时刻，结点 1 的状态接受来自结点 3 的上一时刻的隐藏状态，因为结点 1 与结点 3相邻。直到 TnTn 时刻，各个结点隐藏状态收敛，每个结点后面接一个 gg 即可得到该结点的输出 oo。</p>
<p>对于不同的图来说，收敛的时刻可能不同，因为收敛是通过两个时刻pp-范数的差值是否小于某个阈值 ϵϵ来判定的，比如：</p>
<p><span class="math display">\[
||Ht+1||2−||Ht||2&lt;ϵ
\]</span></p>
<h3 id="时序卷积神经网络">3.5 时序卷积神经网络</h3>
<h4 id="tcn">TCN</h4>
<p>卷积核大小的限制使得卷积不能很好的抓取长时的依赖信息，因而对于时序问题的建模较多采用由于循环自回归而对时间序列的表示较好的循环神经网络（RNN）。 但一些特殊的卷积神经网络结构在时序建模上也有良好的表现，比如Goolgle用于语音合成的wavenet，Facebook提出的用于翻译的卷积神经网络特殊的卷积神经网络——时序卷积网络（Temporal convolutional network， TCN）与多种RNN结构相对比，发现在多种任务上TCN都能达到甚至超过RNN模型。</p>
<p>TCN相比RNN有并行性、感受野尺寸可以灵活调整、梯度稳定、占用内存更低、相比LSTM更快的特点；与之相对的，这种单向结构的TCN 在迁移学习方面适应能力较弱，并且在实际应用中仍旧受限于卷积神经网络普遍存在的因感受野而带来的某些限制。</p>
<h4 id="deeptcn">DeepTCN</h4>
<p>DeepTCN是一种基于卷积神经网络的多关联时间序列预测的概率预测模型。该模型可用于估计参数和非参数设置下的概率密度。其构建了基于膨胀因果卷积网的叠加残差块来捕获序列的时间依赖性。与表示学习相结合，能够学习复杂的模式，如季节性、系列内和跨系列的假日效应，并利用这些模式进行更准确的预测，特别是在历史数据稀少或不可用的情况下。</p>
<p>1.提出了一种基于卷积的预测框架，该框架同时提供了估计概率密度的参数和非参数方法。</p>
<p>2.该框架能够学习序列间的潜在相关性，能够处理数据稀疏、冷启动等复杂的实际预测情况，具有较高的可扩展性和可扩展性。</p>
<p>3.大量的实证研究表明，我们的框架优于其他最先进的方法，无论是点预测和概率预测。</p>
<p>4.与递归结构相比，卷积模型的计算可以完全并行化，从而达到较高的训练效率。与此同时，优化要容易得多。在我们的案例中，训练时间是文献Flunkert等(2017)报道的复发模型的1/8。</p>
<p>5.该模型非常灵活，可以包括外生协变量，如额外的促销计划或天气预报。</p>
<h3 id="卷积神经网络中的其他上采样方法">3.6 卷积神经网络中的其他上采样方法</h3>
<h5 id="pixelshuffle">3.6.1 PixelShuffle</h5>
<p>PixelShuffle由Redmon等人在CVPR2016中提出的一种上采样方法，其主要关注于亚像素卷积层。首先将一个H x W的低分辨率图像通过三次卷积之后变为通道数为<span class="math inline">\(r^2\)</span>的特征图，然后用reshape的方法将 Hx W x r2 的特征图重组为 rH x rW 的高分辨率图像。 经过三次卷积之后输出与原图一样尺寸的r2通道的输出图，扩大的倍数刚刚好等同于通道数。这样可以让网络学习到插值方法，并存在于前三层卷积层参数中[3.6.1-1]。</p>
<p><img src="https://img-blog.csdnimg.cn/2019080116323060.png" srcset="/img/loading.gif" lazyload=""></p>
<p>PixelShuffle相对超分辨率此前不同的超分辨率的优点在于：不需要通过线性插值来扩大特征图尺寸，以更小的卷积核获得很好的效果；并且用卷积学习比手工设计插值会有更好的拟合效果。</p>
<h5 id="dupsampling">3.6.2 DUpsampling</h5>
<p>​ DUpsampling是在CVPR2019年提出的一种上采样方法[3.6.2-1]，从网络结构来看和PixelShuffle有相似之处，其通过卷积学习亚像素，并最后重组来获得更大的图像。</p>
<p>DUpsampling在对特征图的操作上有所不同，是先通过将单个像素所对应的C个通道reshape成一个1xC的向量，与CxN的矩阵相乘得到1xN的向量，再reshape成为2x2xN/4（2应该指的是放大倍数，即rxrxN/r2）的扩大后的亚像素块,组合成放大后的特征图。 以上这两种算法都是基于数据去训练的，可以获得比线性插值更好的效果，但是与线性插值相比存在的问题是：1、对于不同的放大倍数的图像需要训练不同的网络（因为通道数的改变）；2、不容易进行连续的放大，比如1.1倍，1.2倍这样。说不容易而不是不能是因为可以适当放大输入图像或者对权重和步长进行调整后放缩，但是计算很复杂，效果没有整数倍好，线性插值却很容易办到。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122144815796.png" srcset="/img/loading.gif" lazyload=""></p>
<h5 id="meta-upscale">3.6.3 Meta-Upscale</h5>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122144856779.png" srcset="/img/loading.gif" lazyload=""></p>
<p>[^ ]: 原来的方法要如何做一次非整数尺度的放缩，这样方便进一步了解Meta-SR的思路。</p>
<p>Meta-SR中的Meta-Upscale是在CVPR2019上的Meta-SR: A Magnifification-Arbitrary Network for Super-Resolution一文中提出来的，作者提出了一种可以任意尺度缩放的方法，可以实现较好效果的非整数的放缩。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122144934656.png" srcset="/img/loading.gif" lazyload=""></p>
<p>FLR 表示由特征学习模块提取的特征，并假定缩放因子是 r。对于 SR 图像上的每个像素（i, j），文中认为它由 ILR 图像上像素（i′,j′）的特征与一组相应卷积滤波器的权重所共同决定。从这一角度看，放大模块可视为从 FLR 到 ISR 的映射函数。 <span class="math display">\[
I^{SR}(i,j)=\Phi(F^{LR}(i^{'},j^{'}),W(i,j))
\]</span> k * k为设定的与I^SR所相关的一个像素搜索范围。及这个范围内的所有像素都是和最后输出的像素是有关系的，需要对其计算相应的权重并加权计算。HWx（InCxoutC）我的理解是图像最后输出的长x宽x输入的通道数（文中为64）x输出的通道数（文中为3）做为权值生成的输出的通道数。对于一个像素的计算我们需要对64个通道上feature map上的对应9个值加权求和，就可以计算出输出的一个通道值，重复3次就是一个值的3个通道数的值，重复HW次就可以计算出整个输出。 那么知道最后如何计算输出之后，我们关心的是文中提到的是怎么来做对不同的放缩大小的权值计算的以及如何在任意尺度下完成输出像素和LR特征图上的对应。 作者把 Meta-Upscale 模块由三个重要的函数，即 Location Projection、Weight Prediction、Feature Mapping（这个就是上文讲的如何乘以权值得到最后的输出）。 先说下文中的Location Projection，如下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122145040963.png" srcset="/img/loading.gif" lazyload=""></p>
<p><span class="math display">\[
(i^{'},j^{'} )=T(i,j)=([\frac{i}{r}],[\frac{j}{r}])
\]</span> 通过向下取整使得ISR中的每一个值都可以在ILR上找到一个对应的值。如放大1.5倍，那么ILR上的0对应ISR中的0和1（0/1.5&lt;1,1/1.5&lt;1）。</p>
<p>而权重预测Weight Prediction在网络中是通过构建了两层全链接层和relu来实现的，输出在上文已经讲过了就不再重复，而文中用于预测的输入是这样获得的，如下式： <span class="math display">\[
V_{ij}=(\frac{i}{r}-[\frac{i}{r}],\frac{j}{r}-[\frac{j}{r}],\frac{1}{r})
\]</span> 总结一下，Meta-SR中的思路有点像RPN，做了一个外部的推荐网络，来推荐放缩的权值及像素的对应关系，在非整数倍放大上起的不错的效果。</p>
<h5 id="capafe">3.6.4 CAPAFE</h5>
<p>​ CAPAFE是出自ICCV2019的一种内容感知并重组特征的上采样方法，结构思路与此前的上采样方法较多的不同之处。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122145107859.png" srcset="/img/loading.gif" lazyload=""></p>
<p>CAPAFE的整个网络结构由两部分组成，一部分是核预测模块（Kernel Prediction Module），用于生成用于重组计算的核上的权重。另一部分是内容感知重组模块（Content-aware Reassembly Module），用于将计算到的权重将通道reshape成一个kxk的矩阵作为核与原本输入的特征图上的对应点及以其为中心点的kxk区域做卷积计算，获得输出。 首先我们先关注下核预测模块是如何预测出权重值的，这一部分文中分为三部分：通道压缩器（Channel Compressor）、内容编码（Content Encoder）、核归一化（Kernel Normaliaer）。输入的Feature map通过1x1的卷积核，完成对通道数的压缩，文中提及这一步的原因是为了减少计算量，而且文中说通过实验将通道数压缩在64不会影响效果，文中给的Cm为64。之后通过多层Kenconder x Kenconder的卷积核完成对特征图的计算，输出HxWxσ2xKup2的特征图，这里Kenconder的大小与感受野有关，越大生成的权重与周围的内容的关联性越高。最后将特征图reshape成σHxσWxKup2，之后单独对每一个像素的所有通道用softmax做归一化，为了是最后生成的重组核内权重值和为1。这样权重的预测就完成了。 之后将预测好的权重对每一个像素都拉成kupxkup的卷积核即图中对应的Wl，通过对（i/σ，j/σ）向下映射的方法（floor function）找到每个像素在原来的feature map上对应的点，以其为中心点构建出kupxkup的区域做卷积计算，就可以得到一个像素的输出，重复操作可以获得整个输出。 <span class="math display">\[
X^{'}_{l'}=\sum^{r}_{n=-r}\sum^{r}_{m=-r}W_{l'(n,m)}*X_{(i+n,j+m)}
\]</span> CAPAFE的整个网络内的参数量特别少，只有Content Encoder中的NxKenconder x Kenconderxσ2xKup2的卷积核的参数，同时它的上采样思路也比较不同，当然它无法如Meta-SR那样做连续缩放。</p>
<h2 id="循环神经网络中的图像采样">4.循环神经网络中的图像采样</h2>
<p>循环神经网络(RNN)也被称为递归神经网络，相比卷积神经网络，其有更悠久的发展历史[4-1]。其被大量用于语音识别[4-2]、机器翻译、文本/音乐生成、情感分类、DNA序列分析、视频行为识别、实体名字识别、场景标记[4-3]，可见其同样在视觉任务中有一定作用。</p>
<h3 id="lstm">LSTM</h3>
<h4 id="lstm采样">4.1 LSTM采样</h4>
<p>LSTM常用于自然语言处理（NLP），但其作为RNN最为经典的模型同样也可以完成图像分类任务。</p>
<p>在每个时间步，LSTM 都使用了 softmax 输出了一个在字符集上的概率分布。</p>
<p>给定每个时间步的分布，有几种方法来获得单个的字符，每个字符获得其相应的嵌入然后传递给 LSTM 作为下一步的输入：</p>
<ol type="1">
<li>获得当前时间步最大的值</li>
<li>从分布中使用 softmax 采样</li>
<li>获取前 k 个结果，使用 beam 搜索</li>
</ol>
<h4 id="评价采样的质量">评价采样的质量</h4>
<p>最简单的方法：可视化数据！诸如“the”或者“and”这样的词出现得非常频繁，即使就是训练很多的时间。随着训练的继续，样本会越来越像真的英语。</p>
<p>而更加形式化的性能评估就是测量 <strong>perplexity</strong> 在一个新的测试集合上。Perplexity 是测试集的逆概率，使用词的数目正规化。最小化 perplexity 和最大化概率是一样的，所以一个更低的 perplexity 模型能够更好地描述数据。Perplexity 是一种常用的度量语言模型的测度，其定义如下： <span class="math display">\[
PP=P(w_1w_2...w_N)^{\frac{-1}{N}}=(\prod_{i=1}^N \frac{1}{(w_i|w_1w_2...w_{i-1})})
\]</span> 其中的 <span class="math display">\[w_1, ..., w_N\]</span> 是训练数据的整个序列；我们将整个这些看成是一个序列。在 bigram 模型中，条件是截断的，<span class="math inline">\(PP==(\prod_{i=1}^N \frac{1}{(P(w_i|w_{i-1})})^{\frac{1}{N}}\)</span></p>
<p>在实践中，我们没有测试集，所以就不会测量 perplexity，我们训练的字符层的模型。perplexity 通常用在词层的语言模型上。</p>
<p>相比Bi-LSTM，LSTM的结构很适合社交网络等文本分析等并应用于推荐系统。</p>
<h3 id="srcnn">4.2 SRCNN</h3>
<p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211122145141623.png" srcset="/img/loading.gif" lazyload=""></p>
<blockquote>
<p>SRCNN超分辨率重建的流程:</p>
<p>（1）将低分辨率图像使用两/三次差值放大至目标尺寸，得到图中的输入(input)；</p>
<p>（2）将低分辨率图像输入到三层的卷积神经网络。例如在论文中，对YCrCb颜色空间中的Y通道重建，网络形式为(conv1+relu1)—(conv2+relu2)—(conv3)。第一层卷积：卷积核大小为9×9(<span class="math inline">\(f_1×f_1\)</span>)，卷积核数目64(<span class="math inline">\(n_1\)</span>)，输出64张特征图；第二层卷积：卷积核尺寸1×1(<span class="math inline">\(f_2×f_2\)</span>)，卷积核数目32(<span class="math inline">\(n_2\)</span>)，输出32张特征图；第三层卷积：卷积核尺寸5×5(<span class="math inline">\(f_3×f_3\)</span>)，卷积核数目为1(<span class="math inline">\(n_3\)</span>)，输出1张特征图即为最终重建高分辨率图像。</p>
</blockquote>
<h2 id="生成对抗网络中的图像采样">5.生成对抗网络中的图像采样</h2>
<p>生成对抗网络（GAN）是在2014年Goodfellow在提出的一种基于概率与统计理论并借鉴博弈思想的深度学习模型[5-1]，目前已经出现C-GAN、Info-GAN、DC-GAN、W-GAN/W-GAN-GP、f-GAN、Big-GAN、EB-GAN、SR-GAN/ESR-GAN等多种变体[5-2]，其在生成图像数据集、 生成人脸照片 、 生成现实照片 、 生成动画角色 、 图像转换 、 文字-图片转化 、 语义图像-图片转化 、 生成正面人像图片 、 生成新体态 、 图片转表情 、 图片编辑 、 图像融合 、 超分辨率 、视频预测、 3D打印等方面已经有了相应的应用[5-3]。</p>
<h3 id="gan">5.1 GAN</h3>
<p>GAN由服从可以不需要或只需要少量的数据标注即可生成有效的高维数据，其通过生成数据的生成模型和估计样本是训练数据的概率的判别模型之间不断博弈获得高维度的生成数据。</p>
<p>生成模型的训练程序是将判别模型判断错误的概率最大化，当判别模型将来自生成模型的数据判断正确率接近50%时，即认为判别模型已无法判断生成的数据是否正确。在G和D由多层感知器定义的情况下，整个系统可以用反向传播进行训练。在训练或生成样本期间，不需要任何马尔科夫链或展开的近似推理网络。[1] 。</p>
<p><img src="https://auto2dev.coding.net/p/ImageHostingService/d/ImageHostingService/git/raw/master/md/41a59b4bd87f50222e05277057b34ce.png" srcset="/img/loading.gif" lazyload=""></p>
<p>GAN的主要灵感来源于博弈论中零和博弈的思想，应用到深度学习神经网络上来说，就是通过生成网络G（Generator）和判别网络D（Discriminator）不断博弈，进而使G学习到数据的分布，如果用到图片生成上，则训练完成后，G可以从一段随机数中生成逼真的图像。G， D的主要功能是：</p>
<p>● G是一个生成网络，它生成一个随机的噪声z（随机数），通过这个噪声生成图像 。对于给定的输入x和输出y，生成模型学习到其映射关系，其联合分布概率<span class="math inline">\(p(x,y)\)</span>,由贝叶斯定理知在已知<span class="math inline">\(p(x)\)</span>和<span class="math inline">\(p(x|y)\)</span>可得到<span class="math inline">\(p(y|x)\)</span>。</p>
<p>● D是一个判别网络，判别一张图片是不是“真实的”。它的输入参数是x，x代表一张图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片</p>
<p>训练过程中，生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量辨别出G生成的假图像和真实的图像。这样，G和D构成了一个动态的“博弈过程”，最终的平衡点即纳什均衡点。</p>
<h2 id="结论">结论</h2>
<p>没啥好看的。。。</p>
<h2 id="参考文献">参考文献</h2>
<p>[1-01] Lecun Y , Bengio Y , Hinton G . Deep learning[J]. nature, 2015, 521(7553):436.</p>
<p>[1-02] Krizhevsky, A., Sutskever, I. and Hinton, G. (2012) ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems, 25, 1097-1105.</p>
<p>[1-03] Khan A, Sohail A, Zahoora U, et al. A survey of the recent architectures of deep convolutional neural networks[J]. arXiv preprint arXiv:1901.06032, 2019.</p>
<p>[1-04] 袁非牛, 章琳, 史劲亭,等. Theories and Applications of Auto-Encoder Neural Networks: A Literature Survey%自编码神经网络理论及应用综述[J]. 计算机学报, 2019, 042(001):203-230.</p>
<p>[1-05] Sabour, Sara, Nicholas Frosst, and Geoffrey E. Hinton. "Dynamic routing between capsules." <em>Advances in neural information processing systems</em>. 2017.</p>
<p>[1-06] Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term memory." <em>Neural computation</em> 9.8 (1997): 1735-1780.</p>
<p>[1-07] Minar M R , Naher J . Recent Advances in Deep Learning: An Overview[J]. 2018.</p>
<p>[2-01] Keys, Robert. "Cubic convolution interpolation for digital image processing." <em>IEEE transactions on acoustics, speech, and signal processing</em> 29.6 (1981): 1153-1160.</p>
<p>[2-02] 张智邦, 李桂清, 韦国栋,等. 形状插值算法综述[J]. 计算机辅助设计与图形学学报, 2015(08):38-49.</p>
<p>[2-03] Chen, Shenchang Eric, and Lance Williams. "View interpolation for image synthesis." <em>Proceedings of the 20th annual conference on Computer graphics and interactive techniques</em>. 1993.</p>
<p>[2-04] Carey, W. Knox, Daniel B. Chuang, and Sheila S. Hemami. "Regularity-preserving image interpolation." <em>IEEE transactions on image processing</em> 8.9 (1999): 1293-1297.</p>
<p>[2-05] Lehmann, Thomas Martin, Claudia Gonner, and Klaus Spitzer. "Survey: Interpolation methods in medical image processing." <em>IEEE transactions on medical imaging</em> 18.11 (1999): 1049-1075.</p>
<p>[2-06] Hou, Hsieh, and H. Andrews. "Cubic splines for image interpolation and digital filtering." <em>IEEE Transactions on acoustics, speech, and signal processing</em> 26.6 (1978): 508-517.</p>
<p>[2-07] Thévenaz, Philippe, Thierry Blu, and Michael Unser. "Image interpolation and resampling." <em>Handbook of medical imaging, processing and analysis</em> 1.1 (2000): 393-420.</p>
<p>[2-08] Dodgson, Neil A. "Quadratic interpolation for image resampling." <em>IEEE transactions on image processing</em> 6.9 (1997): 1322-1326.</p>
<p>[2-09] Hwang, Jung Woo, and Hwang Soo Lee. "Adaptive image interpolation based on local gradient features." <em>IEEE signal processing letters</em> 11.3 (2004): 359-362.</p>
<p>[2-10] Dong, Weisheng, et al. "Sparse representation based image interpolation with nonlocal autoregressive modeling." <em>IEEE Transactions on Image Processing</em> 22.4 (2013): 1382-1394.</p>
<p>[3-01] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</p>
<p>[3-02] Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</p>
<p>[3-03] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9.</p>
<p>[3-04] He K , Zhang X , Ren S , et al. Deep Residual Learning for Image Recognition[C]// IEEE Conference on Computer Vision &amp; Pattern Recognition. IEEE Computer Society, 2016.</p>
<p>[^]: ResNet</p>
<p>[3-05] Sun, Ke, et al. "Igcv3: Interleaved low-rank group convolutions for efficient deep neural networks." <em>arXiv preprint arXiv:1806.00178</em> (2018).</p>
<p>[^ ]: IGCV3</p>
<p>[3-06] 陈超, 齐峰. 卷积神经网络的发展及其在计算机视觉领域中的应用综述[J]. 计算机科学, 2019, 46(03):69-79.</p>
<p>[3.1-1] Masci, Jonathan, et al. "Stacked convolutional auto-encoders for hierarchical feature extraction." <em>International conference on artificial neural networks</em>. Springer, Berlin, Heidelberg, 2011.</p>
<p>[3.2.1-1] Vincent, O. Rebecca, and Olusegun Folorunso. "A descriptive algorithm for sobel image edge detection." <em>Proceedings of Informing Science &amp; IT Education Conference (InSITE)</em>. Vol. 40. California: Informing Science Institute, 2009.</p>
<p>[3.2.3-1] Zeiler, Matthew D., et al. "Deconvolutional networks." <em>2010 IEEE Computer Society Conference on computer vision and pattern recognition</em>. IEEE, 2010.</p>
<p>[3.2.3-2] Zeiler, Matthew D., Graham W. Taylor, and Rob Fergus. "Adaptive deconvolutional networks for mid and high level feature learning." <em>2011 International Conference on Computer Vision</em>. IEEE, 2011.</p>
<p>[3.2.3-3] Zeiler, Matthew D., and Rob Fergus. "Visualizing and understanding convolutional networks." <em>European conference on computer vision</em>. Springer, Cham, 2014.</p>
<p>[3.2.6-1] Larsson, Gustav, Michael Maire, and Gregory Shakhnarovich. "Fractalnet: Ultra-deep neural networks without residuals." <em>arXiv preprint arXiv:1605.07648</em> (2016).</p>
<p>[3.2.6-2] <a href="https://arxiv.org/abs/1605.07146" target="_blank" rel="noopener">Zagoruyko, Sergey, and Nikos Komodakis. "Wide residual networks." <em>arXiv preprint arXiv:1605.07146</em> (2016).</a></p>
<p>[3.2.6-3] Xie, Saining, et al. "Aggregated residual transformations for deep neural networks." <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2017.</p>
<p>[3.2.6-4] Huang, Gao, et al. "Densely connected convolutional networks." <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2017.</p>
<p>[3.2.6-5] Redmon, Joseph, and Ali Farhadi. "Yolov3: An incremental improvement." <em>arXiv preprint arXiv:1804.02767</em> (2018).</p>
<p>[3-07] [Yu, Fisher, Vladlen Koltun, and Thomas Funkhouser. "Dilated residual networks." <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2017. ](</p>
<p>[3.2.7-1] Yu, Fisher, Koltun, Vladlen. Multi-Scale Context Aggregation by Dilated Convolutions[J].</p>
<p>[3.2.7 -2] Understanding the Effective Receptive Field in Deep Convolutional Neural Networks，Wenjie Luo, Yujia Li, Raquel Urtasun, Richard Zemel</p>
<p>[3.2.8-1] Redmon, Joseph, et al. "You only look once: Unified, real-time object detection." <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2016.</p>
<p>[3.2.8-2] Redmon, Joseph, and Ali Farhadi. "YOLO9000: better, faster, stronger." <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2017.</p>
<p>[3.3-1] Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks for semantic segmentation." <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2015.</p>
<p>Lecun Y , Bottou L . Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):P.2278-2324.</p>
<p>[3.6.1-1] Shi W , Caballero J , Ferenc Huszár, et al. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network[J]. 2016.</p>
<p>[3.6.2-1] Tian, Zhi, et al. "Decoders matter for semantic segmentation: Data-dependent decoding enables flexible feature aggregation." <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2019.</p>
<p>[3.6.4-3] Hu, Xuecai, et al. "Meta-SR: a magnification-arbitrary network for super-resolution." <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2019.</p>
<p>[3.6.4-1] Wang, Jiaqi, et al. "CARAFE: Content-Aware ReAssembly of FEatures." <em>Proceedings of the IEEE International Conference on Computer Vision</em>. 2019.</p>
<p>[4-1] Salehinejad, Hojjat, et al. "Recent advances in recurrent neural networks." <em>arXiv preprint arXiv:1801.01078</em> (2017).</p>
<p>[4-2] Deng, Li, Geoffrey Hinton, and Brian Kingsbury. "New types of deep neural network learning for speech recognition and related applications: An overview." <em>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</em>. IEEE, 2013.</p>
<p>[4-3] Byeon, Wonmin, et al. "Scene labeling with lstm recurrent neural networks." <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2015.</p>
<p>[5-1] Goodfellow I J , Pouget-Abadie J , Mirza M , et al. Generative Adversarial Networks[J]. Advances in Neural Information Processing Systems, 2014, 3:2672-2680.</p>
<p>[5-2] 朱秀昌. 生成对抗网络图像处理综述[J]. 南京邮电大学学报:自然科学版, 2019, 39(3):1-12.</p>
<p>[5-3] Creswell, Antonia, et al. "Generative adversarial networks: An overview." <em>IEEE Signal Processing Magazine</em> 35.1 (2018): 53-65.</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/">数据科学</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/06/07/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E7%9A%84%E6%BC%94%E5%8F%98%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">大数据技术的演变：计算框架</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/05/10/win10%E5%AE%89%E8%A3%85Hadoop%E4%BB%A5%E5%8F%8Apyspark/">
                        <span class="hidden-mobile">win10安装Hadoop以及pyspark</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     
  </div>
  

  

  



  
    <script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>
    <script>
      if (window.mermaid) {
        mermaid.initialize({theme: 'forest'});
      }
    </script>
  



</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const mathjax = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
